import argparse
import os
import json
import random
import torch
import numpy as np
from tqdm import tqdm
from datasets import load_dataset
from utils import open_config, create_model
from detector.attn import AttentionDetector
from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) 
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def main(args):
    set_seed(args.seed)

    output_logs = f"./result/{args.dataset_name}/{args.model_name}-{args.seed}.json"
    output_result = f"./result/{args.dataset_name}/result.jsonl"
    
    model_config_path = f"./configs/model_configs/{args.model_name}_config.json"
    model_config = open_config(config_path=model_config_path)

    model = create_model(config=model_config)
    model.print_model_info()

    dataset = load_dataset(args.dataset_name)
    test_data = dataset['test']
    
    detector = AttentionDetector(model)
    print("===================")
    print(f"Using detector: {detector.name}")

    labels, predictions, scores = [], [], []
    logs = []

    for data in tqdm(test_data):
        result = detector.detect(data['text'])
        detect = result[0]
        score = result[1]['focus_score']

        labels.append(data['label'])
        predictions.append(detect)
        scores.append(1-score)

        result_data = {
            "text": data['text'],
            "label": data['label'],
            "result": result
        }

        logs.append(result_data)

    auc_score = roc_auc_score(labels, scores)
    auprc_score = average_precision_score(labels, scores)

    tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()
    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0

    auc_score = round(auc_score, 3)
    auprc_score = round(auprc_score, 3)
    fnr = round(fnr, 3)
    fpr = round(fpr, 3)

    print(f"AUC Score: {auc_score}; AUPRC Score: {auprc_score}; FNR: {fnr}; FPR: {fpr}")
    
    os.makedirs(os.path.dirname(output_logs), exist_ok=True)
    with open(output_logs, "w") as f_out:
        f_out.write(json.dumps({"result": logs}, indent=4))

    os.makedirs(os.path.dirname(output_result), exist_ok=True)
    with open(output_result, "a") as f_out:
        f_out.write(json.dumps({
            "model": args.model_name,
            "seed": args.seed,
            "auc": auc_score,
            "auprc": auprc_score,
            "fnr": fnr,
            "fpr": fpr
        }) + "\n")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Prompt Injection Detection Script")
    
    parser.add_argument("--model_name", type=str, default="qwen2-attn",
                        help="Path to the model configuration file.")
    parser.add_argument("--dataset_name", type=str, default="deepset/prompt-injections", 
                        help="Path to the dataset.")
    parser.add_argument("--seed", type=int, default=0)
    
    args = parser.parse_args()

    main(args)